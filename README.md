# LearnNLP
I'm reviewing NLP for my job hunt.  I'll modify this plan as I go with things I found helpful and useful, and include code and resources under each week.

## The Plan
### Week 1-2: Introduction to Natural Language Processing (NLP) and GPT Models
* Read "Speech and Language Processing" by Daniel Jurafsky and James H. Martin (Chapters 1-3) (5 hours) - https://web.stanford.edu/~jurafsky/slp3/
* Read "The Illustrated GPT-2" by Jay Alammar (1 hour) - https://jalammar.github.io/illustrated-gpt2/
* Project: Write a Python script that generates text using pre-trained GPT-2 models from Hugging Face's Transformers library.

### Week 3-4: Introduction to Transformers and Hugging Face Library
* Read "Attention Is All You Need" by Ashish Vaswani et al. (1 hour) - https://arxiv.org/abs/1706.03762
* Read "Hugging Face Transformers" official documentation (4 hours) - https://huggingface.co/transformers/
* Project: Write a Python script that fine-tunes a pre-trained GPT-2 model on a custom text dataset using Hugging Face's Transformers library.

### Week 5-6: Fine-Tuning GPT Models with Hugging Face Library
* Read "Fine-tuning a language model on a text classification task" by Hugging Face (2 hours) - https://huggingface.co/blog/how-to-train
* Read "The Illustrated GPT-2 (Visualizing Transformer Language Models)" by Jay Alammar (1 hour) - https://jalammar.github.io/illustrated-gpt2/
* Project: Write a Python script that fine-tunes a pre-trained GPT-2 model on a specific language modeling task, such as predicting the next word in a sentence or generating text in a specific writing style.

### Week 7-8: Advanced Techniques for Fine-Tuning GPT Models
* Read "How to Fine-Tune BERT for Text Classification?" by Chris McCormick (1 hour) - https://mccormickml.com/2019/07/22/BERT-fine-tuning/
* Read "The Annotated Transformer" by HarvardNLP (3 hours) - https://nlp.seas.harvard.edu/2018/04/03/attention.html
* Project: Write a Python script that fine-tunes a pre-trained BERT model on a text classification task, such as sentiment analysis or topic classification. Implement a custom loss function or regularization technique in the fine-tuning process and evaluate its impact on the model's performance.

### Week 9-10: Advanced Techniques for Training LSTMs and Related Models
* Read "The Unreasonable Effectiveness of Recurrent Neural Networks" by Andrej Karpathy (1 hour) - http://karpathy.github.io/2015/05/21/rnn-effectiveness/
* Read "Sequence Modeling With Neural Networks" by Alex Graves (2 hours) - https://arxiv.org/pdf/1606.07792.pdf
* Project: Implement a more complex LSTM-based model, such as a sequence-to-sequence model or a conditional language model, and train it on a text dataset.

### Week 11-12: Project Work and Practice
* Choose a project that applies your knowledge of GPT models and NLP, such as developing a chatbot or summarization tool.
* Fine-tune a GPT model on a large corpus of text data and compare its performance to other language models or baseline models.
* Note: You can adjust the timeline based on your learning pace and availability. Additionally, these are just suggested resources and projects. You can also refer to Hugging Face's documentation and other online resources for further learning. Good luck with your learning and project work!
